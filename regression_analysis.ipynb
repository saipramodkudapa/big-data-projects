{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "regression-analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_1wM0wOZIif"
      },
      "source": [
        "pip install pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvY7poNcZhyh"
      },
      "source": [
        "import pyspark\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext(\"local\", \"BDA\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJlw90RxZkis"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYNzmcceZui_"
      },
      "source": [
        "import json\n",
        "json_file = sc.textFile(\"Software_5.json.gz\")\n",
        "records = json_file.map(lambda js: json.loads(js))\n",
        "records.take(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkL466JAZ4g3"
      },
      "source": [
        "import re\n",
        "from operator import add\n",
        "def reg_filter(word):\n",
        "    return True if re.match(r'((?:[\\.,!?;\"])|(?:(?:\\#|\\@)?[A-Za-z0-9_\\-]+(?:\\'[a-z]{1,3})?))', word.lower()) else False\n",
        "\n",
        "\n",
        "# Find top 1k common words across reviews using word count algo\n",
        "reviews = records.filter(lambda review: 'reviewText' in review)\n",
        "review_words = reviews.map(lambda review: (review['reviewText'])).flatMap(lambda review: review.split())\n",
        "filtered_review_words = review_words.filter(lambda word: reg_filter(word))\n",
        "word_freq_kvs = filtered_review_words.map(lambda word: (word.lower(), 1)).reduceByKey(add)\n",
        "onek_common_words = word_freq_kvs.sortBy(lambda t: t[1], False).map(lambda t: t[0]).take(1000)\n",
        "onek_common_words_shared = sc.broadcast(onek_common_words)\n",
        "onek_common_words_shared.value\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JXl7nIAL9cp"
      },
      "source": [
        "# Find relative frequency of 1k words \n",
        "# Prepare data for 1k linear regressions grouped by common word\n",
        "def relative_frequencies(review_text):\n",
        "    onek_common_words = onek_common_words_shared.value\n",
        "    words = review_text.split()\n",
        "    qualified_words = []\n",
        "    for w in words:\n",
        "        if reg_filter(w):\n",
        "            qualified_words.append(w.lower())\n",
        "    if len(qualified_words) > 0:\n",
        "        onek_rel_freqs = []\n",
        "        for cw in onek_common_words:\n",
        "            onek_rel_freqs.append((cw, qualified_words.count(cw)/len(qualified_words)))\n",
        "        return onek_rel_freqs\n",
        "    else:\n",
        "        return list(zip(onek_common_words, [0]*1000))\n",
        "\n",
        "review_with_rel_freqs = reviews.map(lambda review: ((review['overall'], int(review['verified'])), relative_frequencies(review['reviewText'])))\n",
        "review_with_rel_freqs_flattened = review_with_rel_freqs.flatMapValues(lambda t: t).map(lambda t: (t[1][0], (t[1][1], t[0][0], t[0][1])))\n",
        "review_with_rel_freqs_grouped_by_word = review_with_rel_freqs_flattened.groupByKey().map(lambda t: (t[0], list(t[1])))\n",
        "review_with_rel_freqs_grouped_by_word.take(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYs0H6W7mWhC"
      },
      "source": [
        "import numpy as np\n",
        "from scipy import stats as ss\n",
        "\n",
        "def compute_beta(list_of_tuples):\n",
        "    ratings = [i[1] for i in list_of_tuples]\n",
        "    rel_freq = [i[0] for i in list_of_tuples]\n",
        "    \n",
        "    mean_X = np.mean(rel_freq)\n",
        "    std_X = np.std(rel_freq)\n",
        "    \n",
        "    normalised_rel_freq = [(i - mean_X)/std_X for i in rel_freq]\n",
        "    \n",
        "    mean_Y = np.mean(ratings)\n",
        "    std_Y = np.std(ratings)\n",
        "    \n",
        "    normalised_ratings = [(i-mean_Y)/std_Y for i in ratings]\n",
        "    \n",
        "    X = np.array(normalised_rel_freq)\n",
        "    \n",
        "    row_to_be_added = np.full((1,len(ratings)), 1)\n",
        "    \n",
        "    X_N = np.transpose(np.vstack((X, row_to_be_added)))\n",
        "\n",
        "    \n",
        "    Y = np.transpose(np.array(normalised_ratings))\n",
        "\n",
        "    beta = np.matmul(np.matmul(np.linalg.inv(np.matmul(np.transpose(X_N),X_N)),np.transpose(X_N)),Y)\n",
        "    \n",
        "    y_pred = beta[1] + beta[0] * X\n",
        "    y = np.array(normalised_ratings)\n",
        "    rss = np.sum(np.square(y_pred - y ))\n",
        "    N = len(ratings)\n",
        "    m = 1\n",
        "    dof = N-(m+1)\n",
        "    s_square = rss/dof\n",
        "    \n",
        "    deno = [np.square(i - mean_X) for i in rel_freq]\n",
        "    deno_sum = np.sum(deno)\n",
        "    standard_error = s_square/np.sqrt(deno_sum)\n",
        "    t_value = beta[0] / standard_error\n",
        "    \n",
        "    t_cdf = ss.t.cdf(t_value, dof)\n",
        "    if t_cdf < 0.5:\n",
        "        p_value =  t_cdf *2\n",
        "    else:\n",
        "        p_value = (1- t_cdf)*2\n",
        "    \n",
        "    return [beta[1], beta[0], p_value]\n",
        "\n",
        "\n",
        "betas_p_values = review_with_rel_freqs_grouped_by_word.map(lambda t: (t[0], compute_beta(t[1])))\n",
        "t20_positive_correlated_words = betas_p_values.sortBy(lambda d: d[1][1], False).take(20)\n",
        "t20_negative_correlated_words = betas_p_values.sortBy(lambda d: d[1][1], False).take(20)\n",
        "\n",
        "t20_positive_correlation"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}